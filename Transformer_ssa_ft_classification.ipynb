{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSA없이, 가변길이 데이터로 x,y좌표 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DDPGtrainedAgent_3D_06_22_2024_1810_0.01_2024_06_30_1918.xlsx', 'TD3trainedAgent_06_26_2024_1840_0.01_2024_06_30_2008.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# directory/folder path\n",
    "excel_dir_path = \"C:/Users/user/Desktop/gait_classification/excel_files\"\n",
    "\n",
    "# list to store files\n",
    "excel_files = []\n",
    "\n",
    "# Iterate directory\n",
    "for file_path in os.listdir(excel_dir_path):\n",
    "    # check if current file_path is a file\n",
    "    if os.path.isfile(os.path.join(excel_dir_path, file_path)):\n",
    "        # add filename to list\n",
    "        excel_files.append(file_path)\n",
    "print(excel_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.decomposition import SingularSpectrumAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1차원 배열을 SSA해서 trend와 periodic을 추출하여 리턴하는 함수\n",
    "def do_SSA(lst):\n",
    "    window_size = 20\n",
    "    ssa = SingularSpectrumAnalysis(window_size=window_size)\n",
    "    components = ssa.fit_transform(np.array(lst).reshape(1,-1))\n",
    "    trend = components[0, 0, :]\n",
    "    periodic = components[0, 1, :]\n",
    "    noise=lst-(trend+periodic)\n",
    "    return trend, periodic, noise\n",
    "\n",
    "# SSA로 얻은 trend와 periodic에서 주파수 성분을 추출하여 단일값을 리턴하는 함수(peak)\n",
    "def fourier_transform(trend, periodic, noise):\n",
    "    time=np.linspace(0, 10, len(trend)) # 10초 simulation\n",
    "    trend_frequency_domain = np.fft.fft(trend)\n",
    "    periodic_frequency_domain = np.fft.fft(periodic)\n",
    "    noise_frequency_domain = np.fft.fft(noise)\n",
    "\n",
    "    #trend_frequencies = np.fft.fftfreq(len(time), time[1] - time[0])\n",
    "    trend_magnitude = np.abs(trend_frequency_domain)\n",
    "\n",
    "    #periodic_frequencies = np.fft.fftfreq(len(time), time[1] - time[0])\n",
    "    periodic_magnitude = np.abs(periodic_frequency_domain)\n",
    "\n",
    "    noise_magnitude = np.abs(noise_frequency_domain)\n",
    "\n",
    "    trend_peak_freq = np.fft.fftfreq(len(time), time[1] - time[0])[np.argmax(trend_magnitude)]\n",
    "    periodic_peak_freq = np.fft.fftfreq(len(time), time[1] - time[0])[np.argmax(periodic_magnitude)]\n",
    "    noise_peak_freq = np.fft.fftfreq(len(time), time[1] - time[0])[np.argmax(noise_magnitude)]\n",
    "\n",
    "    return trend_peak_freq, periodic_peak_freq, noise_peak_freq\n",
    "\n",
    "# SSA로 얻은 trend와 periodic에서 주파수 성분을 추출하여 단일값을 리턴하는 함수(total)\n",
    "def fourier_transform_total(trend, periodic, noise):\n",
    "    time=np.linspace(0, 10, len(trend)) # 10초 simulation\n",
    "    trend_frequency_domain = np.fft.fft(trend)\n",
    "    periodic_frequency_domain = np.fft.fft(periodic)\n",
    "    noise_frequency_domain = np.fft.fft(noise)\n",
    "\n",
    "    #trend_frequencies = np.fft.fftfreq(len(time), time[1] - time[0])\n",
    "    trend_magnitude = np.abs(trend_frequency_domain)\n",
    "\n",
    "    #periodic_frequencies = np.fft.fftfreq(len(time), time[1] - time[0])\n",
    "    periodic_magnitude = np.abs(periodic_frequency_domain)\n",
    "    noise_magnitude = np.abs(noise_frequency_domain)\n",
    "\n",
    "    trend_spectral_energy = np.sum(trend_magnitude**2)\n",
    "    periodic_spectral_energy = np.sum(periodic_magnitude**2)\n",
    "    noise_spectral_energy = np.sum(noise_magnitude**2)\n",
    "\n",
    "    return trend_spectral_energy, periodic_spectral_energy, noise_spectral_energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DDPG\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "filename=excel_files[0]\n",
    "\n",
    "def get_sheetnames_xlsx(filepath):\n",
    "    wb = load_workbook(filepath, read_only=True, keep_links=False)\n",
    "    return wb.sheetnames\n",
    "\n",
    "sheet_names=get_sheetnames_xlsx(excel_dir_path+\"/\"+filename)\n",
    "\n",
    "DDPG_list=[]\n",
    "for i in range(len(sheet_names)):\n",
    "    df=pd.read_excel(excel_dir_path+\"/\"+filename, sheet_name=sheet_names[i], engine='openpyxl', index_col=None, header=None)\n",
    "    \n",
    "    # NaN 값을 제거한 후 SSA, fourier transform\n",
    "    compressed_columns = [fourier_transform(do_SSA(df[col].dropna().tolist())[0], do_SSA(df[col].dropna().tolist())[1], do_SSA(df[col].dropna().tolist())[2]) for col in df.columns]\n",
    "    list_trend = [tup[0] for tup in compressed_columns]\n",
    "    list_periodic = [tup[1] for tup in compressed_columns]\n",
    "    list_noise = [tup[2] for tup in compressed_columns]\n",
    "\n",
    "    DDPG_list.append(list_trend)\n",
    "    DDPG_list.append(list_periodic)\n",
    "    DDPG_list.append(list_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TD3\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "filename=excel_files[1]\n",
    "\n",
    "def get_sheetnames_xlsx(filepath):\n",
    "    wb = load_workbook(filepath, read_only=True, keep_links=False)\n",
    "    return wb.sheetnames\n",
    "\n",
    "sheet_names=get_sheetnames_xlsx(excel_dir_path+\"/\"+filename)\n",
    "\n",
    "TD3_list=[]\n",
    "for i in range(len(sheet_names)):\n",
    "    df=pd.read_excel(excel_dir_path+\"/\"+filename, sheet_name=sheet_names[i], engine='openpyxl', index_col=None, header=None)\n",
    "    \n",
    "    # NaN 값을 제거한 후 SSA, fourier transform\n",
    "    compressed_columns = [fourier_transform(do_SSA(df[col].dropna().tolist())[0], do_SSA(df[col].dropna().tolist())[1], do_SSA(df[col].dropna().tolist())[2]) for col in df.columns]\n",
    "    list_trend = [tup[0] for tup in compressed_columns]\n",
    "    list_periodic = [tup[1] for tup in compressed_columns]\n",
    "    list_noise = [tup[2] for tup in compressed_columns]\n",
    "\n",
    "    TD3_list.append(list_trend)\n",
    "    TD3_list.append(list_periodic)\n",
    "    TD3_list.append(list_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 141)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_list=[DDPG_list[i]+TD3_list[i] for i in range(len(TD3_list))]\n",
    "# (141(feature 개수), 200(simulation 개수)) 3차원인 시계열 데이터에서 2차원 단일값 데이터로 변환\n",
    "\n",
    "final_array=np.array(final_list)\n",
    "\n",
    "final_array=final_array.reshape(final_array.shape[1], final_array.shape[0]) #각 instance마다 141개의 feature\n",
    "\n",
    "final_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_array\n",
    "target=[0]*100+[1]*100\n",
    "y=np.array(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from Model import *\n",
    "from Utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'numpy.float64\\'>\"})'}), <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m\n\u001b[0;32m     12\u001b[0m test_model1\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mts_output\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m},\n\u001b[0;32m     13\u001b[0m                                 optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m),\n\u001b[0;32m     14\u001b[0m                                 metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mts_output\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     15\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, restore_best_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[0;32m     17\u001b[0m training_history_test_model1 \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtest_model1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#validation_data=(XTest, YTest),\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m plotTrainingProgress(training_history\u001b[38;5;241m=\u001b[39mtraining_history_test_model1, \n\u001b[0;32m     28\u001b[0m                         title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest model1 multivariate Transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1080\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1084\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[0;32m   1086\u001b[0m     )\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1092\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'numpy.float64\\'>\"})'}), <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "test_model1=build_multivariate_transformer_classification_model( # multivariate transformer model for gait forecasting\n",
    "    look_back=141, n_features=1,\n",
    "    num_classes=2,\n",
    "    head_size=512,\n",
    "    num_heads=8,\n",
    "    ff_dim=8,\n",
    "    num_transformer_blocks=8,\n",
    "    mlp_units=[512, 256, 64], \n",
    "    dropout=0.1,\n",
    "    mlp_dropout=0.1)\n",
    "\n",
    "test_model1.compile(loss = {\"ts_output\" : 'mean_squared_error'},\n",
    "                                optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                                metrics = {\"ts_output\" : 'mean_absolute_error'})\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience = 30, restore_best_weights = True)]\n",
    "\n",
    "training_history_test_model1 = \\\n",
    "test_model1.fit(\n",
    "    final_list, \n",
    "    y,    \n",
    "    #validation_data=(XTest, YTest),\n",
    "    epochs = 200,\n",
    "    batch_size = 32, \n",
    "    callbacks = callbacks,\n",
    "    verbose = 2)\n",
    "\n",
    "plotTrainingProgress(training_history=training_history_test_model1, \n",
    "                        title=\"Test model1 multivariate Transformer\")\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m training_history_test_model2 \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtest_model1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFinal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#validation_data=(XTest, YTest),\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m plotTrainingProgress(training_history\u001b[38;5;241m=\u001b[39mtraining_history_test_model2, \n\u001b[0;32m     12\u001b[0m                         title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest model1 multivariate Transformer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "training_history_test_model2 = \\\n",
    "test_model1.fit(\n",
    "    Final, \n",
    "    y,    \n",
    "    #validation_data=(XTest, YTest),\n",
    "    epochs = 200,\n",
    "    batch_size = 32, \n",
    "    callbacks = callbacks,\n",
    "    verbose = 2)\n",
    "\n",
    "plotTrainingProgress(training_history=training_history_test_model2, \n",
    "                        title=\"Test model1 multivariate Transformer\")\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
